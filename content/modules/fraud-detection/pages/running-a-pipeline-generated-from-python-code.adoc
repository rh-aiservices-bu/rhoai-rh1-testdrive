[id='running-a-pipeline-generated-from-python-code']
= Running a data science pipeline generated from Python code
include::_attributes.adoc[]

In the previous section, you created a simple pipeline by using the GUI pipeline editor. +
It's often desirable to create pipelines by using code that can be version-controlled and shared with others. +
The https://github.com/kubeflow/kfp-tekton[kfp-tekton,window=_blank] SDK provides a Python API for creating pipelines. +
The SDK is available as a Python package that you can install by using the `pip install kfp-tekton` command. +
With this package, you can use Python code to create a pipeline and then compile it to Tekton YAML. +
Then you can import the YAML code into {productname-short}.

This {deliverable} does not delve into the details of how to use the SDK. Instead, it provides the files for you to view and upload.

. Optionally, view the provided Python code in your Jupyter environment by navigating to the `fraud-detection/pipeline` folder. It contains the following files:
+
* `5_get_data_train_upload.py` is the main pipeline code.
* `get_data.py`, `train_model.py`, and `upload.py` are the three components of the pipeline.
* `build.sh` is a script that builds the pipeline and creates the YAML file.`
+
. The generated `5_get_data_train_upload.yaml` file is located one level up, in the `fraud-detection` directory.

. Right-click the `5_get_data_train_upload.yaml` file and then click *Download*.
+
[.bordershadow]
image::pipelines/wb-download.png[Download Pipeline YAML,400]

. Go back to your project in the OpenShift AI Dashboard
. Upload the `5_get_data_train_upload.yaml` file to {productname-short}.

.. In the Data Science dashboard, navigate to your data science project page and then click *Import pipeline*.
+
[.bordershadow]
image::pipelines/dsp-pipeline-import.png[]

.. Provide values:
*** for *Pipeline name*: `Python pipeline`
*** for *Pipeline description*: `Pipeline written in Python and converted using kftp-tekton`

.. Click *Upload* and then select `5_get_data_train_upload.yaml` from your local files to upload the pipeline.
+
[.bordershadow]
image::pipelines/dsp-pipline-import-upload.png[, 600]

.. Click *Import pipeline* to import and save the pipeline.
+
. The pipeline shows in the list of pipelines but does not execute. This is normal

. Expand the pipeline item
+
[.bordershadow]
image::pipelines/dsp-pipeline-expand.png[Expand Pipeline]

. Click *Create run*.
+
[.bordershadow]
image::pipelines/dsp-pipeline-create-run.png[Create Pipeline Run]

. On the *Create run* page, enter a *Name*. You can leave the other fields with their default values.
+
[.bordershadow]
image::pipelines/pipeline-create-run-form.png[Create Pipeline Run form]

. Click *Create* to create the run.
+
A new run starts immediately and opens the run details page.
+
[.bordershadow]
image::pipelines/pipeline-run-in-progress.png[]

There you have it:  a pipeline created in Python, now running in {productname-short}.
